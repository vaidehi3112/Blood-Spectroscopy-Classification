{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b86c7e-4203-4d3b-9a4f-b370c550baf5",
   "metadata": {},
   "source": [
    "# Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7dc133-6c5d-4d88-ba0a-60655cc05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Libraries used for Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Sampling\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Grid Search\n",
    "from sklearn.experimental import enable_halving_search_cv  \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85960a85-2704-4a13-889e-d55ca5203893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d269c7e3-af8b-43df-8a4a-1e1305b239b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(train):\n",
    "    train2 = train\n",
    "    index_vals = train2.shape[0]\n",
    "    index_vals = set(range(index_vals))\n",
    "    skipped = []\n",
    "    \n",
    "    for colname in train2.columns:\n",
    "        # IQR\n",
    "        if colname in ['Reading_ID', 'donation_id', 'id', \n",
    "                       'hdl_cholesterol_human', \n",
    "                       'hemoglobin(hgb)_human', \n",
    "                       'cholesterol_ldl_human']:\n",
    "            #Skip the ID column and Target variables in outlier detection\n",
    "            continue\n",
    "            \n",
    "        #Caluculating quartile values\n",
    "        Q1 = np.percentile(train2[colname], 25,\n",
    "                       interpolation = 'midpoint')\n",
    "        Q3 = np.percentile(train2[colname], 75,\n",
    "                       interpolation = 'midpoint')\n",
    "        \n",
    "        #Calculating IQR values\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Upper bound\n",
    "        upper = np.where(train2[colname] >= (Q3+1.5*IQR))\n",
    "        # Lower bound\n",
    "        lower = np.where(train2[colname] <= (Q1-1.5*IQR))\n",
    "\n",
    "        up = index_vals.intersection(set(upper[0]))\n",
    "        index_vals = index_vals-set(upper[0])\n",
    "        lo = index_vals.intersection(set(lower[0]))\n",
    "        index_vals = index_vals-set(lower[0])\n",
    "\n",
    "        try:\n",
    "            train2.drop(list(up), inplace = True)\n",
    "            train2.drop(list(lo), inplace = True)\n",
    "        except:\n",
    "            #Handling no outliers\n",
    "            skipped.append(colname)\n",
    "        \n",
    "    return train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a196ee-b24c-4bd0-af2e-04fcbcfba058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliersCols(train,cols):\n",
    "    train2=train\n",
    "    index_vals=train2.shape[0]\n",
    "    index_vals=set(range(index_vals))\n",
    "    skipped=[]\n",
    "    for colname in cols:\n",
    "        # IQR\n",
    "        if colname in ['Reading_ID', 'donation_id', 'id', \n",
    "                       'hdl_cholesterol_human', 'hemoglobin(hgb)_human', \n",
    "                       'cholesterol_ldl_human']:\n",
    "            continue\n",
    "            #Skip the ID column and Target variables in outlier detection\n",
    "            \n",
    "        #Caluculating quartile values\n",
    "        Q1 = np.percentile(train2[colname], 25,\n",
    "                       interpolation = 'midpoint')\n",
    "        Q3 = np.percentile(train2[colname], 75,\n",
    "                       interpolation = 'midpoint')\n",
    "        \n",
    "        #Calculating IQR values\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Upper bound\n",
    "        upper = np.where(train2[colname] >= (Q3+1.5*IQR))\n",
    "        # Lower bound\n",
    "        lower = np.where(train2[colname] <= (Q1-1.5*IQR))\n",
    "\n",
    "        up=index_vals.intersection(set(upper[0]))\n",
    "        index_vals=index_vals-set(upper[0])\n",
    "        lo=index_vals.intersection(set(lower[0]))\n",
    "        index_vals=index_vals-set(lower[0])\n",
    "\n",
    "        try:\n",
    "            train2.drop(list(up), inplace = True)\n",
    "            train2.drop(list(lo), inplace = True)\n",
    "        except:\n",
    "            #Handling no outliers\n",
    "            skipped.append(colname)\n",
    "        \n",
    "    return train2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9dae8-45a0-412c-a8f3-763868656743",
   "metadata": {},
   "source": [
    "### SMOTE Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052116c1-c3e5-42e1-a782-0827bb02e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Sampling for each label\n",
    "def overSampleData(train, labels_n, target):\n",
    "    trainS = train.iloc[:,0:173].to_numpy()\n",
    "    # summarize class distribution\n",
    "    print('Samples per class before oversampling minority class:')\n",
    "    counter = Counter(labels_n[:,target])\n",
    "    print(counter)\n",
    "    \n",
    "    # transform the dataset\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(trainS, labels_n[:,0])\n",
    "    \n",
    "    # summarize the new class distribution\n",
    "    print('Samples per class after oversampling minority class:')\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18430d70-22a6-44b7-8dc7-b4e87fb3e868",
   "metadata": {},
   "source": [
    "# Dimenssionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3ee9e-0637-44f8-9575-6cfd3853ac05",
   "metadata": {},
   "source": [
    "### 1. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bd0943-d1cc-4cb1-b88c-a269e873e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform RFE and fit Random Forest\n",
    "def RFE_DimReduction(X, labels_n, targetVal, noFeaturesToSelect, \n",
    "                     validation, validationLabels_n, test):\n",
    "    #targetVal denotes the target which we wish to predit\n",
    "    # i.e. Hdl_cholesterol_human, Cholesterol_ldl_human, \n",
    "    #Hemoglobin(hgb)_human\n",
    "    \n",
    "    y = labels_n[:, targetVal]\n",
    "    estimator = SVR(kernel = \"linear\")\n",
    "    selector = RFE(estimator, \n",
    "                   n_features_to_select = noFeaturesToSelect, \n",
    "                   step = 1)\n",
    "    selector = selector.fit(X, y)\n",
    "    \n",
    "    support = selector.support_\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    features = X\n",
    "    noCols = features.shape\n",
    "    dropFeatures = []\n",
    "    for i in range(noCols[1]):\n",
    "        if(support[i] == False):\n",
    "            dropFeatures.append(i)\n",
    "    \n",
    "    #Dropping the features after performing RFE\n",
    "    updatedFeatures = np.delete(features, dropFeatures, 1) \n",
    "    updatedValidation = np.delete(validation, dropFeatures, 1) \n",
    "    updatedTest = np.delete(test, dropFeatures, 1) \n",
    "    \n",
    "    print(\"Feature Indexes to be dropped: \")\n",
    "    print(dropFeatures)\n",
    "    \n",
    "    print(\"Train Data: \", updatedFeatures.shape)\n",
    "    print(\"Validation Data: \", updatedValidation.shape)\n",
    "    print(\"Test Data: \", updatedTest.shape)\n",
    "    \n",
    "    return updatedFeatures, updatedValidation, updatedTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d245680-9238-4e26-91c4-e5382cc48227",
   "metadata": {},
   "source": [
    "### 2. Recursive Feature Elimination Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638b0d56-9f20-429e-ae15-ae5a5849ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFECV_DimReduction(X, labels_n, targetVal, noFeaturesToSelect, \n",
    "                       validation, validationLabels_n, test):\n",
    "    #targetVal denotes the target which we wish to predit\n",
    "    # i.e. Hdl_cholesterol_human, Cholesterol_ldl_human, \n",
    "    #Hemoglobin(hgb)_human\n",
    "    \n",
    "    y = labels_n[:, targetVal]\n",
    "    estimator = SVR(kernel = \"linear\")\n",
    "    selector = RFECV(estimator, step = 1)\n",
    "    selector = selector.fit(X, y)\n",
    "    \n",
    "    support = selector.support_\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    features = X\n",
    "    dropFeatures = []\n",
    "    for i in range(173):\n",
    "        if(support[i] == False):\n",
    "            dropFeatures.append(i)\n",
    "    \n",
    "    #Dropping the features after performing RFE\n",
    "    updatedFeatures = np.delete(features, dropFeatures, 1) \n",
    "    updatedValidation = np.delete(validation, dropFeatures, 1) \n",
    "    updatedTest = np.delete(test, dropFeatures, 1) \n",
    "    \n",
    "    print(\"Feature Indexes to be dropped: \")\n",
    "    print(dropFeatures)\n",
    "    \n",
    "    print(\"Train Data: \", updatedFeatures.shape)\n",
    "    print(\"Validation Data: \", updatedValidation.shape)\n",
    "    print(\"Test Data: \", updatedTest.shape)\n",
    "    \n",
    "    return updatedFeatures, updatedValidation, updatedTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312db123-15bf-4f32-b4fe-775dcd9ade9d",
   "metadata": {},
   "source": [
    "### 3. Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f32d24c-298b-4bf2-b8df-d332b7f684d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyPCA(features, labels_n, nofeature, validation, validationLabels_n, \n",
    "              test, noFeatures):\n",
    "    pca = PCA(n_components = noFeatures)\n",
    "    pca.fit(features)\n",
    "    updatedFeatures = pca.transform(features)\n",
    "        \n",
    "    print(\"Dimenssion of feature array after applying PCA: \", \n",
    "          updatedFeatures.shape)\n",
    "    updatedValidation = pca.transform(validation)\n",
    "    print(\"Dimenssion of validation array after applying PCA: \", \n",
    "          updatedValidation.shape)\n",
    "    updatedTest = pca.transform(test)\n",
    "    print(\"Dimenssion of test array after applying PCA: \", \n",
    "          updatedTest.shape)\n",
    "    \n",
    "    return updatedFeatures, updatedValidation, updatedTest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
